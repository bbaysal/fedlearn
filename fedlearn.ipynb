{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Importing needed libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pylab as plt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "source": [
    "Defining hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_nodes specifies number of edges in the network. \n",
    "number_of_nodes = 10\n",
    "\n",
    "# Traditional NN layer dimensions. First value should be equal to input dimensions. \n",
    "input_dim= 28*28\n",
    "layers_dims = [input_dim, 200,  10]\n",
    "len_layers= len(layers_dims)\n",
    "learning_rate = 0.1\n",
    "epoch = 1000"
   ]
  },
  {
   "source": [
    "Defining NN model. Traditional MLP network. Layer size and number of neurons in layer can be specified with layer_dim array. Additionally, parameters can be get and set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers_size):\n",
    "\n",
    "        # layer_size parameter defines MLP dimensions. For example if layer_size [784, 50, 50, 10] then \n",
    "        # network has 3 layer and input dimension should be match with first value of array.\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)-1\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    "\n",
    "        A = X.T\n",
    "        \n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + \\\n",
    "                self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "\n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + \\\n",
    "            self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "\n",
    "        return A, store\n",
    "    \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, Y, store):\n",
    "\n",
    "        derivatives = {}\n",
    "\n",
    "        store[\"A0\"] = X.T\n",
    "\n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    "\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "\n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "\n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    "\n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "\n",
    "        return derivatives\n",
    "\n",
    "    def get_all_parameters(self, name):\n",
    "        return self.parameters[name]\n",
    "\n",
    "    def get_parameter(self, name):\n",
    "        return self.parameters[name]\n",
    "\n",
    "    def set_parameter(self, name, value):\n",
    "        self.parameters[name] = value\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, n_iterations=2500, model_name=\"\"):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        self.n = X.shape[0]\n",
    "\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "\n",
    "            cost = -np.mean(Y * np.log(A.T + 1e-8))\n",
    "\n",
    "            derivatives = self.backward(X, Y, store)\n",
    "\n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    "\n",
    "            if loop % 100 == 0:\n",
    "                acc = self.predict(X, Y)\n",
    "                if model_name is not \"\":\n",
    "                    print(\"Epoch: \", int(n_iterations/100), \" / \",\n",
    "                          int(loop/100), \" --- Model:\", model_name)\n",
    "                else:\n",
    "                    print(\"Epoch: \", int(n_iterations/100), \" / \", int(loop/100))\n",
    "\n",
    "                print(\"Model: \", model_name, \" Cost: \",\n",
    "                      cost, \"Train Accuracy:\", acc)\n",
    "\n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    "\n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()"
   ]
  },
  {
   "source": [
    "Methods used in federated learning process\n",
    "\n",
    "3 methods are used for distrubuting dataset over nodes independent and identically. First method, shuffle labels and their indexes accordingly. Second method distrubutes this label sets to nodes and at the last method also images are get and store set and label in dictionary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method shuffles labels at the beginning and store their indexes separately\n",
    "def split_and_shuffle_labels(y_data, seed, amount):\n",
    "    y_data = pd.DataFrame(y_data)\n",
    "    y_data[\"i\"] = np.arange(len(y_data))\n",
    "    label_dict = dict()\n",
    "    for i in range(10):\n",
    "        var_name = \"label\" + str(i)\n",
    "        label_info = y_data[y_data[\"class\"] == str(i)]\n",
    "        np.random.seed(seed)\n",
    "        label_info = np.random.permutation(label_info)\n",
    "        label_info = label_info[0:amount]\n",
    "        label_info = pd.DataFrame(label_info, columns=[\"labels\", \"i\"])\n",
    "        label_dict.update({var_name: label_info})\n",
    "    return label_dict\n",
    "\n",
    "# The ındexes that grouped previous method are distributed to node datasets equally\n",
    "def get_iid_subsamples_indices(label_dict, number_of_nodes, amount):\n",
    "    sample_dict = dict()\n",
    "\n",
    "    for i in range(number_of_nodes):\n",
    "        sample_name = \"node\"+str(i)\n",
    "        dumb = pd.DataFrame()\n",
    "        for j in range(10):\n",
    "            label_name = str(\"label\")+str(j)\n",
    "            total = len(label_dict[label_name])\n",
    "            batch_size = int(total/number_of_nodes)\n",
    "            a = label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb = pd.concat([dumb, a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)\n",
    "        sample_dict.update({sample_name: dumb})\n",
    "    return sample_dict\n",
    "\n",
    "# Creating datasets for each node according to the indexes getting at previous method. \n",
    "# Images at the related index and labels are transformed to dataset.\n",
    "def create_iid_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict = dict()\n",
    "    y_data_dict = dict()\n",
    "\n",
    "\n",
    "    for i in range(len(sample_dict)): \n",
    "        xname = x_name+str(i)\n",
    "        yname = y_name+str(i)\n",
    "        node_name = \"node\"+str(i)\n",
    "\n",
    "\n",
    "        indices = np.sort(\n",
    "            np.array(sample_dict[node_name][\"i\"], dtype=np.int32))\n",
    "\n",
    "        x_info = np.asfarray(x_data)[indices, :]\n",
    "        y_info = np.asfarray(y_data)[indices]\n",
    "\n",
    "        # Preprocessing before going to be dataset.\n",
    "        x_info, y_info = pre_process_data(x_info, y_info)\n",
    "\n",
    "        x_data_dict.update({xname: x_info})\n",
    "        y_data_dict.update({yname: y_info})\n",
    "\n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "source": [
    "After preparing the datasets, nodes will be created and stored in dictionary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(number_of_nodes):\n",
    "    model_dict = dict()\n",
    "\n",
    "    for i in range(number_of_nodes):\n",
    "        model_name = \"model\"+str(i)\n",
    "        model_info = NN(layers_dims)\n",
    "        model_dict.update({model_name: model_info})\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "source": [
    "All weights of nodes are averaging with this method. At first step, zero-filled matrix are created for addition and all weights and biases are summed afterwards. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avaraged_weights(model_dict, number_of_nodes):\n",
    "    avarages_dict = dict()\n",
    "\n",
    "    # Loop over number of layer in network\n",
    "    # For the first step, zero-fill matrix are created only. \n",
    "    for i in range(1, len(layers_dims)):\n",
    "        model = \"model0\"\n",
    "        weight_name = \"W\"+str(i)\n",
    "        bias_name = \"b\"+str(i)\n",
    "        weight = np.zeros(model_dict[model].get_parameter(weight_name).shape)\n",
    "        bias = np.zeros(model_dict[model].get_parameter(bias_name).shape)\n",
    "        avarages_dict.update({weight_name: weight})\n",
    "        avarages_dict.update({bias_name: bias})\n",
    "\n",
    "    # Loop over all nodes and alsı all layers. \n",
    "    for i in range(number_of_nodes):\n",
    "        model = \"model\" + str(i)\n",
    "        for j in range(1, len(layers_dims)):\n",
    "            weight_name = \"W\"+str(j)\n",
    "            bias_name = \"b\"+str(j)\n",
    "\n",
    "            weight = avarages_dict[weight_name] + \\\n",
    "                model_dict[model].get_parameter(weight_name)\n",
    "            bias = avarages_dict[bias_name] + \\\n",
    "                model_dict[model].get_parameter(bias_name)\n",
    "\n",
    "            avarages_dict.update({weight_name: weight})\n",
    "            avarages_dict.update({bias_name: bias})\n",
    "\n",
    "    # Averaging weights and biases at the end.\n",
    "    for i in range(1, len(layers_dims)):\n",
    "        weight_name = \"W\"+str(i)\n",
    "        bias_name = \"b\"+str(i)\n",
    "        weight = avarages_dict[weight_name] / number_of_nodes\n",
    "        bias = avarages_dict[bias_name] / number_of_nodes\n",
    "        avarages_dict.update({weight_name: weight})\n",
    "        avarages_dict.update({bias_name: bias})\n",
    "\n",
    "    return avarages_dict\n",
    "\n",
    "# Updating main model parameters according to average weights of nodes. \n",
    "def set_main_model_weights(main_model, model_dict):\n",
    "    avg_dict = get_avaraged_weights(model_dict, len(model_dict))\n",
    "\n",
    "    for i in range(1, len(layers_dims)):\n",
    "        weight_name = \"W\"+str(i)\n",
    "        bias_name = \"b\"+str(i)\n",
    "        main_model.set_parameter(weight_name, avg_dict[weight_name])\n",
    "        main_model.set_parameter(bias_name, avg_dict[bias_name])\n",
    "\n",
    "    return main_model\n",
    "\n",
    "# Send main model parameters to all nodes. \n",
    "def set_node_weights(main_model, model_dict, len_layers):\n",
    "    for i in range(len(model_dict)):\n",
    "        model_name = \"model\" + str(i)\n",
    "        for j in range(1, len_layers):\n",
    "            weight_name = \"W\"+str(j)\n",
    "            bias_name = \"b\"+str(j)\n",
    "            model_dict[model_name].set_parameter(\n",
    "                weight_name, main_model.get_parameter(weight_name))\n",
    "            model_dict[model_name].set_parameter(\n",
    "                bias_name, main_model.get_parameter(bias_name))\n",
    "    return model_dict\n",
    "\n",
    "def test_nodes_and_main_model(main_model, model_dict, test_x_dict, test_y_dict):\n",
    "    nodes = len(model_dict)\n",
    "    table = pd.DataFrame(np.zeros([nodes, 3]), columns=[\n",
    "        \"node\", \"node_model\", \"main_model\"])\n",
    "    for i in range(nodes):\n",
    "        test_input_name = \"test_x\" + str(i)\n",
    "        test_target_name = \"test_y\" + str(i)\n",
    "        X = test_x_dict[test_input_name]\n",
    "        y = test_y_dict[test_target_name]\n",
    "        model_name = \"model\" + str(i)\n",
    "        model = model_dict[model_name]\n",
    "\n",
    "        acc = model.predict(X, y)\n",
    "        main_acc = main_model.predict(X, y)\n",
    "\n",
    "        table.loc[i, \"node\"] = \"Node \"+str(i)\n",
    "        table.loc[i, \"node_model\"] = acc\n",
    "        table.loc[i, \"main_model\"] = main_acc\n",
    "\n",
    "    return table\n",
    "\n",
    "# Training all nodes.\n",
    "def train_nodes(model_dict, train_x_dict, train_y_dict):\n",
    "    model_costs=dict()\n",
    "    for i in range(len(model_dict)):\n",
    "        model_name = \"model\"+str(i)\n",
    "        train_input_name = \"train_x\" + str(i)\n",
    "        train_target_name = \"train_y\" + str(i)\n",
    "        X = train_x_dict[train_input_name]\n",
    "        y = train_y_dict[train_target_name]\n",
    "\n",
    "        model = model_dict[model_name]\n",
    "        model.fit(X, y, learning_rate=learning_rate,\n",
    "                  n_iterations=epoch, model_name=model_name)\n",
    "        model_costs.update({model_name: model.costs})\n",
    "        model_dict.update({model_name: model})\n",
    "    return model_costs\n",
    "\n",
    "# Preprocessing on the MNIST data.\n",
    "# Images are normalized and labels converted to one hot encoding\n",
    "def pre_process_data(set_x, set_y):\n",
    "    # Normalize\n",
    "    set_x = set_x / 255.\n",
    "\n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    set_y = enc.fit_transform(set_y.reshape(len(set_y), -1))\n",
    "\n",
    "    return set_x, set_y\n"
   ]
  },
  {
   "source": [
    "Preparing and training part of model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     node  node_model  main_model\n0  Node 0    9.626437   11.925287\n1  Node 1    9.051724   11.925287\n2  Node 2    9.913793   11.925287\n3  Node 3   10.344828   11.925287\n4  Node 4    9.051724   11.925287\n5  Node 5   12.356322   11.925287\n6  Node 6    8.189655   11.925287\n7  Node 7    9.913793   11.925287\n8  Node 8    9.913793   11.925287\n9  Node 9    9.339080   11.925287\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "set_node_weights() missing 1 required positional argument: 'len_layers'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4cda71a9f22c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Setting node weights same as main model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mset_node_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: set_node_weights() missing 1 required positional argument: 'len_layers'"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, data_home=\"./data/\")\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Prepearing raw dataset to federated learning.\n",
    "label_dict = split_and_shuffle_labels(train_y, 1, train_y.shape[0])\n",
    "sample_dict = get_iid_subsamples_indices(\n",
    "    label_dict, number_of_nodes,  train_y.shape[0])\n",
    "train_x_dict, train_y_dict = create_iid_subsamples(\n",
    "    sample_dict, train_x, train_y, \"train_x\", \"train_y\")\n",
    "\n",
    "label_dict = split_and_shuffle_labels(test_y, 1, test_y.shape[0])\n",
    "sample_dict = get_iid_subsamples_indices(\n",
    "    label_dict, number_of_nodes,  test_y.shape[0])\n",
    "test_x_dict, test_y_dict = create_iid_subsamples(\n",
    "    sample_dict, test_x, test_y, \"test_x\", \"test_y\")\n",
    "\n",
    "# Main model the netowrk. \n",
    "main_model = NN(layers_dims)\n",
    "\n",
    "# Nodes will be created\n",
    "model_dict = create_nodes(number_of_nodes)\n",
    "\n",
    "# Accuracy values before training \n",
    "\n",
    "table = test_nodes_and_main_model(main_model, model_dict, test_x_dict, test_y_dict)\n",
    "print(table)\n",
    "\n",
    "# Setting node weights same as main model\n",
    "set_node_weights(main_model, model_dict)\n",
    "\n",
    "# Training\n",
    "model_costs= train_nodes(model_dict, train_x_dict, train_y_dict)\n",
    "\n",
    "# Adjusting  main model weights  with average value\n",
    "set_main_model_weights(main_model, model_dict)\n",
    "\n",
    "# Accuract values after training. Now model and node values are differeny. \n",
    "table = test_nodes_and_main_model(main_model, model_dict, test_x_dict, test_y_dict)\n",
    "print(table)\n",
    "\n",
    "# setting average value all throughtout network\n",
    "set_node_weights(main_model, model_dict)\n",
    "\n",
    "# Accuract values after setting weights to average\n",
    "table = test_nodes_and_main_model(main_model, model_dict, test_x_dict, test_y_dict)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols=2\n",
    "n_rows=int(number_of_nodes/n_cols)\n",
    "# Creating 2x2 plot area\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12,12))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.7)\n",
    "for ax,(name, value) in zip(axes.reshape(-1), model_costs.items()):\n",
    "  ax.plot(value)\n",
    "  ax.set(xlabel='Epoch', ylabel='Loss', title=name)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ]
}